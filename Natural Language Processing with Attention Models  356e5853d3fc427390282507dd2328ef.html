<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Natural Language Processing with Attention Models DeepLearning.AI</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-translucentGray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="356e5853-d3fc-4273-9028-2507dd2328ef" class="page sans"><header><h1 class="page-title"><strong><strong>Natural Language Processing with Attention Models DeepLearning.AI</strong></strong></h1><table class="properties"><tbody><tr class="property-row property-row-status"><th><span class="icon property-icon"><svg viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesStatus"><path d="M8.75488 1.02344C8.75488 0.613281 8.41309 0.264648 8.00293 0.264648C7.59277 0.264648 7.25098 0.613281 7.25098 1.02344V3.11523C7.25098 3.51855 7.59277 3.86719 8.00293 3.86719C8.41309 3.86719 8.75488 3.51855 8.75488 3.11523V1.02344ZM3.91504 5.0293C4.20215 5.31641 4.69434 5.32324 4.97461 5.03613C5.26855 4.74902 5.26855 4.25684 4.98145 3.96973L3.53906 2.52051C3.25195 2.2334 2.7666 2.21973 2.47949 2.50684C2.19238 2.79395 2.18555 3.28613 2.47266 3.57324L3.91504 5.0293ZM10.9629 4.01758C10.6826 4.30469 10.6826 4.79688 10.9697 5.08398C11.2568 5.37109 11.749 5.36426 12.0361 5.07715L13.4854 3.62793C13.7725 3.34082 13.7725 2.84863 13.4785 2.55469C13.1982 2.27441 12.7061 2.27441 12.4189 2.56152L10.9629 4.01758ZM15.0234 8.78906C15.4336 8.78906 15.7822 8.44727 15.7822 8.03711C15.7822 7.62695 15.4336 7.28516 15.0234 7.28516H12.9385C12.5283 7.28516 12.1797 7.62695 12.1797 8.03711C12.1797 8.44727 12.5283 8.78906 12.9385 8.78906H15.0234ZM0.975586 7.28516C0.56543 7.28516 0.223633 7.62695 0.223633 8.03711C0.223633 8.44727 0.56543 8.78906 0.975586 8.78906H3.07422C3.48438 8.78906 3.83301 8.44727 3.83301 8.03711C3.83301 7.62695 3.48438 7.28516 3.07422 7.28516H0.975586ZM12.0361 10.9902C11.749 10.71 11.2568 10.71 10.9629 10.9971C10.6826 11.2842 10.6826 11.7764 10.9697 12.0635L12.4258 13.5127C12.7129 13.7998 13.2051 13.793 13.4922 13.5059C13.7793 13.2256 13.7725 12.7266 13.4854 12.4395L12.0361 10.9902ZM2.52051 12.4395C2.22656 12.7266 2.22656 13.2188 2.50684 13.5059C2.79395 13.793 3.28613 13.7998 3.57324 13.5127L5.02246 12.0703C5.31641 11.7832 5.31641 11.291 5.03613 11.0039C4.74902 10.7168 4.25684 10.71 3.96973 10.9971L2.52051 12.4395ZM8.75488 12.9658C8.75488 12.5557 8.41309 12.207 8.00293 12.207C7.59277 12.207 7.25098 12.5557 7.25098 12.9658V15.0576C7.25098 15.4609 7.59277 15.8096 8.00293 15.8096C8.41309 15.8096 8.75488 15.4609 8.75488 15.0576V12.9658Z"></path></svg></span>Status</th><td><span class="status-value select-value-color-blue"><div class="status-dot status-dot-color-blue"></div>In Progress</span></td></tr><tr class="property-row property-row-person"><th><span class="icon property-icon"><svg viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesPerson"><path d="M10.9536 7.90088C12.217 7.90088 13.2559 6.79468 13.2559 5.38525C13.2559 4.01514 12.2114 2.92017 10.9536 2.92017C9.70142 2.92017 8.65137 4.02637 8.65698 5.39087C8.6626 6.79468 9.69019 7.90088 10.9536 7.90088ZM4.4231 8.03003C5.52368 8.03003 6.42212 7.05859 6.42212 5.83447C6.42212 4.63843 5.51245 3.68945 4.4231 3.68945C3.33374 3.68945 2.41846 4.64966 2.41846 5.84009C2.42407 7.05859 3.32251 8.03003 4.4231 8.03003ZM1.37964 13.168H5.49561C4.87231 12.292 5.43384 10.6074 6.78711 9.51807C6.18628 9.14746 5.37769 8.87231 4.4231 8.87231C1.95239 8.87231 0.262207 10.6917 0.262207 12.1628C0.262207 12.7974 0.548584 13.168 1.37964 13.168ZM7.50024 13.168H14.407C15.4009 13.168 15.7322 12.8423 15.7322 12.2864C15.7322 10.8489 13.8679 8.88354 10.9536 8.88354C8.04492 8.88354 6.17505 10.8489 6.17505 12.2864C6.17505 12.8423 6.50635 13.168 7.50024 13.168Z"></path></svg></span>Assign</th><td><span class="user"><img src="https://lh3.googleusercontent.com/a/AEdFTp5GQgEfwpvN2gSnLXHQHvlEwMmn9Xo1l_2yiPkk=s100" class="icon user-icon"/>nada essam</span></td></tr><tr class="property-row property-row-date"><th><span class="icon property-icon"><svg viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesDate"><path d="M3.29688 14.4561H12.7031C14.1797 14.4561 14.9453 13.6904 14.9453 12.2344V3.91504C14.9453 2.45215 14.1797 1.69336 12.7031 1.69336H3.29688C1.82031 1.69336 1.05469 2.45215 1.05469 3.91504V12.2344C1.05469 13.6973 1.82031 14.4561 3.29688 14.4561ZM3.27637 13.1162C2.70898 13.1162 2.39453 12.8154 2.39453 12.2207V5.9043C2.39453 5.30273 2.70898 5.00879 3.27637 5.00879H12.71C13.2842 5.00879 13.6055 5.30273 13.6055 5.9043V12.2207C13.6055 12.8154 13.2842 13.1162 12.71 13.1162H3.27637ZM6.68066 7.38086H7.08398C7.33008 7.38086 7.41211 7.30566 7.41211 7.05957V6.66309C7.41211 6.41699 7.33008 6.3418 7.08398 6.3418H6.68066C6.44141 6.3418 6.35938 6.41699 6.35938 6.66309V7.05957C6.35938 7.30566 6.44141 7.38086 6.68066 7.38086ZM8.92285 7.38086H9.31934C9.56543 7.38086 9.64746 7.30566 9.64746 7.05957V6.66309C9.64746 6.41699 9.56543 6.3418 9.31934 6.3418H8.92285C8.67676 6.3418 8.59473 6.41699 8.59473 6.66309V7.05957C8.59473 7.30566 8.67676 7.38086 8.92285 7.38086ZM11.1582 7.38086H11.5547C11.8008 7.38086 11.8828 7.30566 11.8828 7.05957V6.66309C11.8828 6.41699 11.8008 6.3418 11.5547 6.3418H11.1582C10.9121 6.3418 10.8301 6.41699 10.8301 6.66309V7.05957C10.8301 7.30566 10.9121 7.38086 11.1582 7.38086ZM4.44531 9.58203H4.84863C5.09473 9.58203 5.17676 9.50684 5.17676 9.26074V8.86426C5.17676 8.61816 5.09473 8.54297 4.84863 8.54297H4.44531C4.20605 8.54297 4.12402 8.61816 4.12402 8.86426V9.26074C4.12402 9.50684 4.20605 9.58203 4.44531 9.58203ZM6.68066 9.58203H7.08398C7.33008 9.58203 7.41211 9.50684 7.41211 9.26074V8.86426C7.41211 8.61816 7.33008 8.54297 7.08398 8.54297H6.68066C6.44141 8.54297 6.35938 8.61816 6.35938 8.86426V9.26074C6.35938 9.50684 6.44141 9.58203 6.68066 9.58203ZM8.92285 9.58203H9.31934C9.56543 9.58203 9.64746 9.50684 9.64746 9.26074V8.86426C9.64746 8.61816 9.56543 8.54297 9.31934 8.54297H8.92285C8.67676 8.54297 8.59473 8.61816 8.59473 8.86426V9.26074C8.59473 9.50684 8.67676 9.58203 8.92285 9.58203ZM11.1582 9.58203H11.5547C11.8008 9.58203 11.8828 9.50684 11.8828 9.26074V8.86426C11.8828 8.61816 11.8008 8.54297 11.5547 8.54297H11.1582C10.9121 8.54297 10.8301 8.61816 10.8301 8.86426V9.26074C10.8301 9.50684 10.9121 9.58203 11.1582 9.58203ZM4.44531 11.7832H4.84863C5.09473 11.7832 5.17676 11.708 5.17676 11.4619V11.0654C5.17676 10.8193 5.09473 10.7441 4.84863 10.7441H4.44531C4.20605 10.7441 4.12402 10.8193 4.12402 11.0654V11.4619C4.12402 11.708 4.20605 11.7832 4.44531 11.7832ZM6.68066 11.7832H7.08398C7.33008 11.7832 7.41211 11.708 7.41211 11.4619V11.0654C7.41211 10.8193 7.33008 10.7441 7.08398 10.7441H6.68066C6.44141 10.7441 6.35938 10.8193 6.35938 11.0654V11.4619C6.35938 11.708 6.44141 11.7832 6.68066 11.7832ZM8.92285 11.7832H9.31934C9.56543 11.7832 9.64746 11.708 9.64746 11.4619V11.0654C9.64746 10.8193 9.56543 10.7441 9.31934 10.7441H8.92285C8.67676 10.7441 8.59473 10.8193 8.59473 11.0654V11.4619C8.59473 11.708 8.67676 11.7832 8.92285 11.7832Z"></path></svg></span>Due</th><td></td></tr><tr class="property-row property-row-url"><th><span class="icon property-icon"><svg viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesUrl"><path d="M7.69922 10.8945L8.73828 9.84863C7.91797 9.77344 7.34375 9.51367 6.91992 9.08984C5.76465 7.93457 5.76465 6.29395 6.91309 5.14551L9.18262 2.87598C10.3379 1.7207 11.9717 1.7207 13.127 2.87598C14.2891 4.04492 14.2822 5.67188 13.1338 6.82031L11.958 7.99609C12.1768 8.49512 12.2451 9.10352 12.1289 9.62988L14.0908 7.6748C15.7725 6 15.7793 3.62109 14.084 1.92578C12.3887 0.223633 10.0098 0.237305 8.33496 1.91211L5.95605 4.29785C4.28125 5.97266 4.26758 8.35156 5.96289 10.0469C6.36621 10.4434 6.90625 10.7441 7.69922 10.8945ZM8.30078 5.13184L7.26855 6.17773C8.08203 6.25293 8.66309 6.51953 9.08008 6.93652C10.2422 8.09863 10.2422 9.73242 9.08691 10.8809L6.81738 13.1504C5.66211 14.3057 4.03516 14.3057 2.87305 13.1504C1.71094 11.9883 1.71777 10.3545 2.87305 9.20605L4.04199 8.03027C3.83008 7.53125 3.75488 6.92969 3.87109 6.39648L1.91602 8.35156C0.234375 10.0264 0.227539 12.4121 1.92285 14.1074C3.61816 15.8027 5.99707 15.7891 7.67188 14.1143L10.0439 11.7354C11.7256 10.0537 11.7324 7.6748 10.0371 5.98633C9.64062 5.58301 9.10059 5.28223 8.30078 5.13184Z"></path></svg></span>Notes</th><td></td></tr><tr class="property-row property-row-multi_select"><th><span class="icon property-icon"><svg viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesMultipleSelect"><path d="M1.91602 4.83789C2.44238 4.83789 2.87305 4.40723 2.87305 3.87402C2.87305 3.34766 2.44238 2.91699 1.91602 2.91699C1.38281 2.91699 0.952148 3.34766 0.952148 3.87402C0.952148 4.40723 1.38281 4.83789 1.91602 4.83789ZM5.1084 4.52344H14.3984C14.7607 4.52344 15.0479 4.23633 15.0479 3.87402C15.0479 3.51172 14.7607 3.22461 14.3984 3.22461H5.1084C4.74609 3.22461 4.45898 3.51172 4.45898 3.87402C4.45898 4.23633 4.74609 4.52344 5.1084 4.52344ZM1.91602 9.03516C2.44238 9.03516 2.87305 8.60449 2.87305 8.07129C2.87305 7.54492 2.44238 7.11426 1.91602 7.11426C1.38281 7.11426 0.952148 7.54492 0.952148 8.07129C0.952148 8.60449 1.38281 9.03516 1.91602 9.03516ZM5.1084 8.7207H14.3984C14.7607 8.7207 15.0479 8.43359 15.0479 8.07129C15.0479 7.70898 14.7607 7.42188 14.3984 7.42188H5.1084C4.74609 7.42188 4.45898 7.70898 4.45898 8.07129C4.45898 8.43359 4.74609 8.7207 5.1084 8.7207ZM1.91602 13.2324C2.44238 13.2324 2.87305 12.8018 2.87305 12.2686C2.87305 11.7422 2.44238 11.3115 1.91602 11.3115C1.38281 11.3115 0.952148 11.7422 0.952148 12.2686C0.952148 12.8018 1.38281 13.2324 1.91602 13.2324ZM5.1084 12.918H14.3984C14.7607 12.918 15.0479 12.6309 15.0479 12.2686C15.0479 11.9062 14.7607 11.6191 14.3984 11.6191H5.1084C4.74609 11.6191 4.45898 11.9062 4.45898 12.2686C4.45898 12.6309 4.74609 12.918 5.1084 12.918Z"></path></svg></span>Tags</th><td><span class="selected-value select-value-color-yellow">Course</span></td></tr><tr class="property-row property-row-url"><th><span class="icon property-icon"><svg viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesUrl"><path d="M7.69922 10.8945L8.73828 9.84863C7.91797 9.77344 7.34375 9.51367 6.91992 9.08984C5.76465 7.93457 5.76465 6.29395 6.91309 5.14551L9.18262 2.87598C10.3379 1.7207 11.9717 1.7207 13.127 2.87598C14.2891 4.04492 14.2822 5.67188 13.1338 6.82031L11.958 7.99609C12.1768 8.49512 12.2451 9.10352 12.1289 9.62988L14.0908 7.6748C15.7725 6 15.7793 3.62109 14.084 1.92578C12.3887 0.223633 10.0098 0.237305 8.33496 1.91211L5.95605 4.29785C4.28125 5.97266 4.26758 8.35156 5.96289 10.0469C6.36621 10.4434 6.90625 10.7441 7.69922 10.8945ZM8.30078 5.13184L7.26855 6.17773C8.08203 6.25293 8.66309 6.51953 9.08008 6.93652C10.2422 8.09863 10.2422 9.73242 9.08691 10.8809L6.81738 13.1504C5.66211 14.3057 4.03516 14.3057 2.87305 13.1504C1.71094 11.9883 1.71777 10.3545 2.87305 9.20605L4.04199 8.03027C3.83008 7.53125 3.75488 6.92969 3.87109 6.39648L1.91602 8.35156C0.234375 10.0264 0.227539 12.4121 1.92285 14.1074C3.61816 15.8027 5.99707 15.7891 7.67188 14.1143L10.0439 11.7354C11.7256 10.0537 11.7324 7.6748 10.0371 5.98633C9.64062 5.58301 9.10059 5.28223 8.30078 5.13184Z"></path></svg></span>URL</th><td><a href="https://www.coursera.org/learn/attention-models-in-nlp?irclickid=xkfRxdU-nxyNTCGw4XR3FxkWUkATQ8RU-SxORk0&amp;irgwc=1&amp;utm_medium=partners&amp;utm_source=impact&amp;utm_campaign=2757406&amp;utm_content=b2c" class="url-value">https://www.coursera.org/learn/attention-models-in-nlp?irclickid=xkfRxdU-nxyNTCGw4XR3FxkWUkATQ8RU-SxORk0&amp;irgwc=1&amp;utm_medium=partners&amp;utm_source=impact&amp;utm_campaign=2757406&amp;utm_content=b2c</a></td></tr></tbody></table></header><div class="page-body"><h1 id="30d47f9c-cfdb-4ff3-971e-596a6217f602" class=""><details open=""><summary>Week 1</summary></details></h1><div class="indented"><p id="ea84d1e3-a339-40f1-aa9b-25cbd3215326" class=""><mark class="highlight-teal">Introduction:</mark> Training models using RNNs is great but result in vanishing gradients for long sentences, that’s why we will start to use attention based models</p><p id="782e593f-2147-4517-9ab7-c5dc045a5e61" class="">
</p><p id="df4320f0-d57e-438b-bcdb-6012c9c18b08" class=""><mark class="highlight-teal">Neural machine translation or Seq2Seq models:</mark> we are using an encoder and decoder (2 lstms, which solve the vanishing gradient problem) → encoder maps variable-length sequences to fixed-length memory or vectors → input and output don’t have to be of the same length</p><figure id="e54c6de3-bc72-483a-afb7-8202d4ceaf14" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled.png"><img style="width:710px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled.png"/></a></figure><p id="461f78e6-3bce-4c02-aba2-f074cd9578ce" class="">seq2seq architecture</p><figure id="09d1dbd0-ec99-46ce-8c05-e8525e680be2" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%201.png"><img style="width:781px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%201.png"/></a></figure><p id="45659e03-92e8-4ffe-a95c-d361f053e86f" class="">encoder outputs the last hidden state that represents the whole input sequence</p><figure id="d68eb32d-4e11-4786-9d51-fd6ac5cb4bfa" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%202.png"><img style="width:748px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%202.png"/></a></figure><p id="a418c085-da45-4e1f-bf6a-7ccb845afa44" class="">the decoder take this hidden state in the initial step and decodes it step by step</p><figure id="d243f8e7-087b-4d1f-ad0f-637bc65c4cf7" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%203.png"><img style="width:777px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%203.png"/></a></figure><p id="b18d90a4-4889-4075-ba46-2a2166832b2e" class="">this fixed size of the hidden state adds a bottleneck, the model has to represent the whole input in this hidden state</p><p id="5e1aa0f9-60e3-409a-bc19-82d5ff924cc9" class="">
</p><p id="489f2fea-c5b8-4ad9-ae44-a91e2dd6bba7" class="">as sequence size increases, model performance decreases</p><p id="0c05b3e0-8afe-45c9-aff8-d8f38fe88b99" class="">as a solution, we can use all the encoder hidden states, but that’s not efficient</p><figure id="796fb79e-57ab-4b4c-91fb-8bb644a61449" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%204.png"><img style="width:780px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%204.png"/></a></figure><p id="efb3bfe2-b55b-4105-aaae-1da7c9c182ac" class="">the ultimate solution is to add an attention layer to make the model focus on the most important words to give attention to, it gives it a new way to process this information</p><hr id="9a3b9238-785f-485a-a0ac-ab6204dac355"/><p id="140bb224-6f55-4c2d-9a1a-46cadec487e9" class="">
</p><p id="c6ddbc2f-c88f-4ad9-919e-c594bb633ded" class="">
</p><figure id="5f4fe817-389b-4930-8c57-36593a5029d7" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%205.png"><img style="width:673px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%205.png"/></a></figure><p id="ae535de5-fabf-4fa9-b2b2-e63e54a0b08d" class="">the original paper that introduced attention, originally for machine translation</p><figure id="1f4782e2-ebba-4bc5-b819-cc4b8ddf6473" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%206.png"><img style="width:781px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%206.png"/></a></figure><p id="92b5b7d4-0b29-4f10-be51-d86db933e932" class="">
</p><figure id="ebb1131e-71dc-460c-9e8a-a10d805aee3a" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%207.png"><img style="width:633px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%207.png"/></a></figure><p id="9c34d9f4-ef9b-4b2d-8a2b-dedb0c839375" class="">the hidden states are added together and represented by a context vector, but we don’t need all the hidden states to predict the first word, what should we do? that’s when attention comes in, take a weighed sum of the hidden states(give attention to certain words each time), but how are these weights calculated? using the previous hidden state in the decoder</p><figure id="bdf42753-5ed3-44cd-a443-c94d0984a66f" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%208.png"><img style="width:782px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%208.png"/></a></figure><p id="ca1c392b-e3f5-4290-81f1-45e013a89c43" class="">
</p><p id="36d2de8e-57b9-4928-8611-e6297a13fb53" class=""><mark class="highlight-teal">The attention layer:</mark></p><p id="6e17201a-3b70-4652-86f4-03971f0c9cbc" class="">the goal of the attention layer is to output a context vector that contains information about the most important words in the hidden states </p><figure id="85f6d6f1-b5e7-494f-8e30-006fd8184786" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%209.png"><img style="width:763px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%209.png"/></a></figure><p id="a72f2356-6b03-4562-941d-eacd76f8083f" class="">
</p><p id="863878cf-ef19-4c32-a85f-e0fd1d8a9116" class=""><mark class="highlight-teal">LSTMs disadvantage:</mark> data has to be processes sequentially, not in parallel, the longer the text is the more data has to fit in the last hidden state the more information lost</p><p id="2be84c8f-8d2c-449a-bdd5-8cac3cba26c2" class="">
</p><p id="ae3e61bc-7d6c-4714-a76d-17307380bcca" class=""><mark class="highlight-teal"><strong>Queries, Keys, Values, and Attention: </strong></mark></p><figure id="9181fdcd-a2bd-4a8d-84f9-e29bc262926d" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2010.png"><img style="width:664px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2010.png"/></a></figure><p id="541b6645-ece5-431e-97ff-5ddedc20be1b" class="">each is represented as an embedding vector</p><p id="5be1bcd1-60bc-444a-b6fa-d099071d7198" class="">the model can learn which words are the most similar in source and target languages</p><p id="3c2c9999-4d4a-4ad7-aff5-de42e47f7b1a" class="">
</p><p id="a45cb364-23ef-45cc-8da4-7bf2a6041755" class=""><mark class="highlight-teal">how is it done?</mark></p><figure id="0def13ae-76bd-4939-94a8-5af7d2775017" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2011.png"><img style="width:685px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2011.png"/></a></figure><figure id="a360f5d5-4b7f-4f7f-bd50-3d43c146fed2" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2012.png"><img style="width:705px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2012.png"/></a></figure><figure id="c63d1ec8-77dc-4af9-99d1-23933eb8dd57" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2013.png"><img style="width:668px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2013.png"/></a></figure><figure id="ced6a373-2227-433d-bae7-59a2ef54590c" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2014.png"><img style="width:696px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2014.png"/></a></figure><p id="73df29cd-8eb8-40be-bedf-48649d443adf" class="">
</p><hr id="2ada0cff-e4da-48f0-86bc-060d1e277293"/><p id="f4689481-3320-45f1-9cb0-0f44681d283c" class=""><mark class="highlight-teal">NMT (neural machine translation) with attention using Teacher forcing:</mark></p><p id="3a0dd598-84e5-4154-8cce-78010daed4b3" class="">what is teacher forcing?</p><figure id="1a52a561-d11e-42ab-976d-5300b46061a7" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2015.png"><img style="width:759px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2015.png"/></a></figure><p id="13c2b56a-aadb-4e4b-84bd-6f0d50efbb57" class="">the problem that occurs in the beginning of training is that when the model predicts a wrong word, this word is inputted to the decoder in the next step, which make the model predict another wrong word and so on…</p><figure id="a099e892-d679-4588-8e97-a2315ae9d218" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2016.png"><img style="width:767px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2016.png"/></a></figure><p id="dd51fb09-e866-4a9f-a04e-67e7dbec9669" class="">to solve this problem, adding the correct instead of the predicted word forces the model to predict the right word in the next step even if it didn’t in the last one</p><hr id="6ae8025b-076a-4ce8-9ce3-d42b5a53dbfb"/><p id="69909923-899e-4746-b21a-e79fbea9d1d5" class=""><mark class="highlight-teal">BLUE score: BiLingual Evaluation Understudy</mark></p><p id="b5d71cd1-aee9-4ba9-80db-586be67f5b3a" class="">it evaluates the quality of machine translated text by comparing candidate translations to reference human translation (one or more)</p><p id="3edc661d-2899-4248-a50c-318d825eda6b" class="">for 0 to 1</p><p id="62997daa-d962-4e7d-8aeb-5d8e847a6789" class="">the closer to 1 the better it is</p><p id="58c6ed4b-0ab6-413f-8b88-02617633d3b6" class="">we count how many word in the candidate appear in the reference and divide by the total words of the reference <mark class="highlight-red">(precision metric)</mark></p><figure id="55990b6d-12a5-4bbf-bef8-34834a15f217" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2017.png"><img style="width:751px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2017.png"/></a></figure><p id="44f931eb-25b4-4794-8cd3-180c032029ef" class="">
</p><p id="4e418528-fb9e-4678-a5dd-537cb4daee9b" class=""><mark class="highlight-teal">Modified version of the BLUE score:</mark></p><p id="ea8e7af0-848b-4c1e-a6fa-730d447ca712" class="">we remove the word from the reference after it is matched with a word in the candidate</p><figure id="1b1ef964-7c30-4652-94ca-735a4a7d8d10" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2018.png"><img style="width:785px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2018.png"/></a></figure><p id="4a7966e6-fa48-440f-90df-26f5e6892d80" class=""><mark class="highlight-teal">disadvantage:</mark></p><p id="3b9687ff-4d4e-4922-a769-ba10bb43db2c" class="">it doesn’t care about semantics or sentence structure</p><hr id="46fde8e3-596b-4e87-a9e6-384848061007"/><p id="d3f48638-98c9-4b62-b8ea-b9c2926d99b7" class=""><mark class="highlight-teal">ROUGE score: </mark><mark class="highlight-red">Recall</mark><mark class="highlight-teal"> Oriented Understudy of Gisting Evaluation</mark></p><p id="8ac39484-7597-4736-a58a-2e6dd0395c14" class="">cares about how much the human reference appears in the candidate translation</p><p id="2997c250-6be7-4cc1-96b9-49c018ee5ed8" class="">ROUGE-N: count the n-gram overlaps between the candidate and reference and divide by the number of word in the reference</p><figure id="5aa73d82-4518-4b28-ac8d-485a278e2653" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2019.png"><img style="width:772px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2019.png"/></a></figure><p id="46c74c7d-6f91-4337-a6f8-517ed11f5f3f" class="">combine both to get f1 score</p><figure id="1262002d-8127-489f-a4be-8dd8c582f282" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2020.png"><img style="width:779px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2020.png"/></a></figure><p id="97b8681a-88d9-4f63-ba36-44e124876758" class="">the metric is better but it still doesn’t consider the sentence structure and semantics</p><hr id="8b0490a6-b120-4295-b69a-048cae9e1c4d"/><p id="b4d59d7f-ae60-4069-970c-ffbcffdf413d" class=""><mark class="highlight-teal">How to construct a translated sentence (Sampling and Decoding):</mark></p><p id="2f5e9473-4142-4b4a-ade9-17ae0f37f66c" class="">greedy decoding and random sampling</p><p id="6fc332fc-2c33-4ad8-b24f-4c774ff44512" class="">decoding methods exist because generating the token with the highest probability always doesn’t get the best output</p><p id="8fa453aa-09ee-4ee0-bf75-76f716702b54" class="">the output of the softmax is a probability distribution over all the tokens in the vocabulary, the way we use this distribution at each step is what decides the output</p><p id="80582777-18e7-44b9-ba65-656db881deef" class="">greedy decoding selects the most probable word at each step, not the best for longer sequences</p><p id="ce1b2ec0-925f-4d88-aaa6-32fa81b76ba8" class="">random sampling it provides a probability for each word and samples accordingly for each output, often little too random for accurate translation</p><p id="b28a988c-bc35-488d-853e-94de0151f2fa" class="">temperature is a parameter that controls randomness in predictions, from 0 to 1, low to high randomness, the higher the more versatile the tokens can be but the more mistakes it can make</p><p id="303661ea-e64a-4c8a-922e-8498633780c0" class="">
</p><p id="ffd73d9f-1796-4ce2-8085-a2a10a5999d4" class=""><mark class="highlight-yellow">Beam Search:</mark> is a technique that allows you to find the best sequences over a fixed window size known as the beam width</p><figure id="0a3ce022-c654-49d4-83d0-5ac71e8b14f9" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2021.png"><img style="width:764px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2021.png"/></a></figure><p id="e22c82e3-9325-4a88-824f-bb267ff4433e" class="">at each time step we keep the most probable B tokens by using the conditional probability, we get the conditional probability of each word given the past words chosen so far. and then the output is the sentence with the most probability (that we get by multiplying all the conditional probabilities of a sentence)</p><p id="da5a7a34-c1d2-4f84-93fc-126315ae30ed" class="">we use the decoder to get these conditional probabilities</p><figure id="6d38458a-f8a4-4e37-a7b1-cc89ed453dd7" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2022.png"><img style="width:808px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2022.png"/></a></figure><p id="f39ef22c-b7e3-4871-8e3e-4752a74040fe" class="">we run the model B times at each time step</p><p id="5062b1c2-76e9-42cb-86d9-c4cc65ab08bf" class="">Problems: it penalizes long sequences, computationally expensive</p><p id="2a3e2c8c-7d37-448c-a481-2cf665c18ae3" class="">
</p><p id="3d835538-89f1-42ac-a9e9-cf95a36f4b98" class=""><mark class="highlight-yellow">MBR (Minimum Bayes Risk): </mark>generates several candidate translations, assign similarity to every pair using a similarity score like ROUGE, select the sample with the highest average similarity</p><figure id="c7a38944-144f-4c82-9510-b128020bcad6" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2023.png"><img style="width:832px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2023.png"/></a></figure><figure id="e7362ab6-84c7-418b-8f0c-17f6f6d63631" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2024.png"><img style="width:756px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2024.png"/></a></figure><p id="320cf3ca-6058-4226-911d-25db8fb208f1" class="">it gives a more contextually accurate translation than random sampling and greedy decoding</p></div><hr id="7b68bba9-8ed3-4935-8880-c545ab58d100"/><h1 id="8eb645c8-1ddd-4e25-bb2a-98d544abf59f" class=""><details open=""><summary>Week 2</summary></details></h1><div class="indented"><p id="665cbf0e-43c8-455c-a551-efc857232cba" class=""><mark class="highlight-teal"><strong>Transformers vs RNNs</strong></mark></p><p id="8e0da46e-8987-4b24-b076-abeeede000aa" class="">RNNs → no parallel computing, more time, takes T sequential steps to encode the input and as the inputs becomes longer vanishing gradients problems arise and loss of information happens, LSTMs and GRUs hep with this problem but they still struggle with long sequence because of the bottleneck problem</p><figure id="71ecff5f-dc65-4897-b4b8-92c4e51ba16f" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2025.png"><img style="width:768px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2025.png"/></a></figure><p id="611d9c05-fb9f-43cd-8d30-77ec9db04074" class="">
</p><p id="a17b81ae-9561-4a12-8b32-74485a705ba5" class=""><mark class="highlight-teal"><strong>Transformers overview</strong></mark></p><p id="c3d1024a-383d-47f5-82da-b41322b6421e" class="">it all started with the Attention Is All You Need paper</p><p id="a0385c18-071e-40b8-8efc-b55ba07ffccf" class="">explaining the whole architecture of the transformer though the whole week</p><p id="f69d8dcc-0760-4e0c-9d0b-ffe96e7bfe90" class="">
</p><p id="e62af4cb-224a-41f0-88e1-9eaf0b25ae67" class="">some applications:</p><figure id="fdd119ac-cb72-4453-a805-2e05b809395c" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2026.png"><img style="width:796px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2026.png"/></a></figure><p id="b6503f8a-42e0-4275-afe7-f54c4bf691ea" class="">the T5 transformer model can perform many tasks at the same time when given a certain request</p><figure id="96569cb8-e2ac-409e-9a1c-ff93fee19cf0" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2027.png"><img style="width:2358px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2027.png"/></a></figure><p id="0ece166e-dffb-403d-a5f7-4bfe7d122b48" class="">
</p><p id="7c507617-bfe5-4b10-9844-ec9b0b316736" class=""><mark class="highlight-teal">The Transformer Parts</mark></p><p id="088195e2-7ffe-4311-a121-824a4dd90473" class="">1- <strong>Scaled and Dot-Product Attention:</strong><div class="indented"><figure id="94b634a6-d892-4eb8-a660-916dc1395a80" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2028.png"><img style="width:745px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2028.png"/></a></figure><figure id="c6df7372-2003-40e0-b378-92d99d0660e1" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2029.png"><img style="width:739px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2029.png"/></a></figure><figure id="930ae09e-cdac-46f2-ba7e-a28fcba39cb2" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2030.png"><img style="width:745px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2030.png"/></a></figure></div></p><p id="b545cc07-ff5f-4c36-989a-d18b18e9bcf5" class="">2- <strong>Masked Self Attention:</strong><div class="indented"><figure id="84ee6ec5-ae11-4d9f-bf57-ba0c2234bad9" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2031.png"><img style="width:779px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2031.png"/></a></figure><figure id="94e9d7cb-e68d-490c-9968-d608130f2034" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2032.png"><img style="width:725px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2032.png"/></a></figure><figure id="4ba5abce-c177-4ba0-9ddc-ef5b81a3b3bc" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2033.png"><img style="width:797px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2033.png"/></a></figure></div></p><p id="18a72466-2369-41ee-aa90-b1e31e5c6358" class="">3- <strong>Multi-head Attention:</strong><div class="indented"><figure id="31ccb474-c588-478e-8a26-46a78cb4d461" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2034.png"><img style="width:803px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2034.png"/></a></figure><p id="d22a9d91-3853-4e6d-bd1f-fa431c904e6a" class="">allows the model to learn different relationships between queries and keys </p><figure id="c95bafcf-b420-476a-81a2-62690b000dca" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2035.png"><img style="width:1974px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2035.png"/></a></figure><figure id="5c94cced-2626-41ef-b83e-960abb6b8517" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2036.png"><img style="width:765px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2036.png"/></a></figure><p id="9a855386-1af6-4968-ae5d-1e836b1112e1" class="">multi-head attention’s cost can be the same as the single-head one because of parallel computing</p></div></p><p id="f44adbbc-ab22-4d83-9671-58d7d84a1a84" class="">4- <strong>Transformer Decoder</strong></p><figure id="a736a362-254e-4ba1-8fe3-12d464298cfa" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2037.png"><img style="width:836px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2037.png"/></a></figure><figure id="b863edfe-b56c-409d-bec8-c75de6ee61b2" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2038.png"><img style="width:790px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2038.png"/></a></figure><figure id="b4e62294-edd0-4692-9500-8b44d9c1e427" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2039.png"><img style="width:865px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2039.png"/></a></figure><figure id="aa7e8367-ac60-4cb4-9bdb-8bbe4a5ca19d" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2040.png"><img style="width:805px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2040.png"/></a></figure><p id="05cc6b4e-63d2-448a-9bf2-fca0cacfb5e7" class="">
</p></div><hr id="8965bdeb-be96-497c-b3b9-cbf2e0ea232d"/><h1 id="f1e1b680-98a2-4938-b669-33c7cdbba6a1" class=""><details open=""><summary>Week 3</summary></details></h1><div class="indented"><p id="466d69e3-e405-419d-b92d-fe41b87a8f5a" class="">week 3 is about QA (question answering) and Transfer learning, using BERT and T5</p><p id="e7ff6fb6-0e13-49ec-a778-0ea29a49e3b4" class="">QA: <div class="indented"><p id="db7a1568-805b-4252-a8ca-9f56313d8eb4" class="">context based → you take a question and context and outputs the answer within the context</p><p id="050cc9d5-9382-4a3a-b1c5-25b0315f8aef" class="">closed book → only takes a question and comes up with its own answer</p></div></p><figure id="be0197aa-f788-4eab-9ac6-6f842179ae37" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2041.png"><img style="width:859px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2041.png"/></a></figure><figure id="61ea2659-6d2e-4879-83ee-dfa58ab569dc" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2042.png"><img style="width:780px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2042.png"/></a></figure><p id="b8886198-bacc-4535-a3f9-e054c383affe" class="">
</p><p id="ec993580-3cc3-4004-9956-6c9fad7887fd" class=""><mark class="highlight-teal">Transfer learning:</mark></p><figure id="b0775798-57a6-49cb-81e0-a00ab2ce9c9d" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2043.png"><img style="width:875px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2043.png"/></a></figure><p id="11574835-0548-4176-9738-f6cd2f179e43" class="">Feature based: using extracted features from a starter model to train another model that accommodates my task</p><p id="36f8580c-312a-4f19-beaf-c96915a00848" class="">Fine-tuning: taking an existing model and tweaking its weights to fit my data</p><figure id="1142774f-02a4-443d-8227-004a2897241c" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2044.png"><img style="width:836px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2044.png"/></a></figure><p id="7143a530-0273-4721-a7ee-ac9fed50e3e6" class="">
</p><p id="3bd77c3c-d4a5-4238-af8e-e07877b124cf" class="">Self-supervised tasks for causal learning or masked language modeling → the model is initially trained on unlabeled data in a self-supervised way. where the input is constructed as in the image below, this results in a general language pretrained model that is later finetuned on a different corpus of a specific task in the same way, then on a certain task like text classification or QA or summarization</p><figure id="ccba1caa-d1f8-48cf-b2f1-5d8fb0eec7e5" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2045.png"><img style="width:849px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2045.png"/></a></figure><p id="ff5b540d-f665-4d24-b71b-55dc15c0f078" class="">
</p><p id="1ec706a0-5b1b-44d0-b40c-99f450371d35" class=""><mark class="highlight-teal">Evolution of NLP models:</mark></p><figure id="dcce98bd-1fa4-4b10-b675-42fda56820fc" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2046.png"><img style="width:720px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2046.png"/></a></figure><p id="23e88889-fb3c-4580-922d-d14cd614c602" class="">
</p><p id="4187a605-4b13-412e-b7ff-e89b5c77727b" class=""><mark class="highlight-teal"><strong>Bidirectional Encoder Representations from Transformers (BERT)</strong></mark></p><p id="8e0896d9-f4e6-4533-bdef-26373a10f58b" class="">the first transformer that made use of pretraining</p><p id="697dacc0-420a-4b26-b282-a6d0cc161f66" class="">consists of 2 steps: pretraining and fine-tuning</p><figure id="2ae00f00-fdc0-4efb-9ee0-8c757f045482" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2047.png"><img style="width:562px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2047.png"/></a></figure><p id="90dc69ba-9d44-4851-8881-81d59832b7fd" class="">
</p><p id="c7b4b4ef-6faa-4bbe-a841-309d6d55741e" class="">pretraining: masked language modeling MLM masks 15% of the data and predict these tokens using cross-entropy</p><figure id="b0e5126f-d584-426e-8f32-8fb03d96b48d" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2048.png"><img style="width:852px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2048.png"/></a></figure><p id="56c037e7-b2d2-4b28-8f2b-0a35d78fa929" class="">
</p><p id="eadfa832-0f86-4b40-85d7-d6a0a3783dcc" class=""><mark class="highlight-teal">How is the input fed to BERT?</mark></p><p id="0088559c-dbe7-42f8-8a8a-e272f9a5ae80" class="">1-segment embeddings: because of next sentence prediction, indicates each sentence position in the input</p><p id="775639f6-f745-4dd3-86bc-32e62976072a" class="">2-position embeddings: indicates the relative position of each word in the sentence, to map the relationships of words in the sentence</p><p id="5cb57686-2c59-4b4d-8e80-5124ea67146b" class="">3-token embeddings: an embedding that represents each token</p><p id="0b18b1a3-228d-47d5-8db3-598d64cc8766" class="">we take the sum of the 3 and this is the input</p><figure id="8b29d3c9-d87a-4264-a0b1-a23681c4567b" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2049.png"><img style="width:896px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2049.png"/></a></figure><figure id="e435d914-7a0c-4ead-9180-5555833510ad" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2050.png"><img style="width:874px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2050.png"/></a></figure><figure id="dc205d5d-8ce8-4c92-8a1c-9e98e311d027" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2051.png"><img style="width:800px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2051.png"/></a></figure><p id="b012256f-1b0b-4733-afb4-1c419f8b78b5" class="">next sentence prediction is to predict is 2 sentences follow each other or not. so, binary loss</p><p id="43cca3dd-2869-4dba-8c5a-6ecf7ddbf3ee" class="">
</p><p id="407a71da-a4fd-4e1f-b922-8f01af0afa38" class=""><mark class="highlight-teal">Fine-Tuning BERT</mark></p><figure id="57a28060-c9fc-44d0-9441-21d931f899fb" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2052.png"><img style="width:879px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2052.png"/></a></figure><figure id="36c49d17-86d9-44f1-9772-94e8c8a62155" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2053.png"><img style="width:786px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2053.png"/></a></figure><p id="54e102b9-59ce-4ab0-872a-39fb84bb02b3" class="">this is the input and the output can be anything needed like NER, sentiment classification and so on…</p><p id="cfe9b5e6-e4c1-40c1-ab00-e013e9cc943c" class="">
</p><p id="fa14249b-f86d-4f20-b7d8-ec4d05b72d24" class=""><strong><mark class="highlight-teal">T5</mark></strong></p><figure id="11175fc8-5d0b-4ef8-bc89-0480bae6ada7" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2054.png"><img style="width:830px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2054.png"/></a></figure><p id="c0618e61-1205-45a8-843b-e74513c0bffc" class="">can be used for many tasks at the same time using the same trained model</p><figure id="1678f875-655f-4c87-a7b9-d464f542fbbd" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2055.png"><img style="width:765px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2055.png"/></a></figure><p id="7cfd24a3-c11f-4c90-9dad-e139f633b4c9" class="">pretrained using MLM</p><p id="4e060901-a7be-43dd-964e-b0bb2562b0e7" class="">
</p><p id="c444f978-8319-4e95-be67-f346f54251aa" class=""><mark class="highlight-teal"><strong>Multi-Task Training Strategy for T5</strong></mark></p><p id="0f6733c1-f083-404e-9e70-1675cb7c554d" class="">the benefit of it is to make the model perform better on a certain task by fine-tuning given that it was pretrained on many task. so, for example train it for translation, summarization and MLM together and fine-tune it on QA. the model parameters are shared across all the tasks.</p><figure id="96f79b7f-3e76-4455-9049-a2c0f4359fbf" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2056.png"><img style="width:878px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2056.png"/></a></figure><p id="77a9acfa-4037-4870-9a3c-0814723435fe" class="">
</p><p id="b465611c-2cb6-4cb3-be68-06a16b50ad87" class="">append a prefix to make ask the model to perform a certain task</p><figure id="23f5bca1-4eff-4f9f-b964-3679b0429ab4" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2057.png"><img style="width:783px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2057.png"/></a></figure><figure id="8a767248-c00f-4519-a360-d71e6e5be1b4" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2058.png"><img style="width:791px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2058.png"/></a></figure><p id="24136eb2-d499-4e4e-9838-8d21cd2276b5" class="">in the third task, it is trained to predict <em>they</em></p><p id="acaee102-727a-4417-ba64-edbab5193eb4" class=""><mark class="highlight-teal">from the T5 paper:</mark></p><figure id="18db3550-a0c8-4f77-9766-94f679ffbdfc" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2059.png"><img style="width:808px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2059.png"/></a></figure><p id="b19675fd-fd69-4090-bb68-5ea53a84bd53" class=""><mark class="highlight-teal">things discussed in the paper: Adapter layers and Gradual Freezing</mark></p><p id="d1e925ab-d9eb-4106-9d53-888aecda2a3e" class="">Gradual Unfreezing: unfreeze each last layer at once and fine-tune until you are done unfreezing all the layers</p><p id="f4ba4e7b-0ee1-4f87-acb0-2495d28da914" class="">Adapter layers: add a new NN to each feedforward and each block of the transformer, they are designed so that the output dimension matches the input adding no structure change to the output. when fine-tuning, only these adapter layers and layer normalization parameters are updated</p><p id="7ab703bc-5fbb-4ec9-9f93-75a7c49b7e9e" class="">
</p><p id="1bc34bf1-02fe-47fd-ad6a-3922b8c3f34b" class=""><mark class="highlight-teal">The GLUE Benchmark (General language understanding evaluation):</mark></p><p id="14838090-3c19-4131-8fc6-620bed57f0f1" class="">Train, evaluate and analyze NLU systems</p><p id="ab16e686-8d87-4760-acb8-fc5017e3c685" class="">datasets are different with different sizes and for different tasks</p><figure id="09c0629a-09aa-4deb-a985-6aa15a784b24" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2060.png"><img style="width:826px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2060.png"/></a></figure><p id="9bf2f1bd-b133-41a2-8020-c57251d30773" class="">
</p><p id="68645da7-8315-4d6d-a658-549236c2c4d0" class=""><mark class="highlight-teal">Question Answering:</mark></p><figure id="6a59b9e9-9c58-4ed3-a6be-1d54009c4757" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2061.png"><img style="width:821px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2061.png"/></a></figure><p id="272c46da-2eb9-4c40-addd-8ef6f737bd5b" class="">
</p><p id="211a1829-3132-4bab-aff1-0fecc5b2151b" class=""><mark class="highlight-pink">Intro to HuggingFace </mark>✔️</p></div><hr id="09df7fe9-200e-424f-aad8-a78aa6489128"/><h1 id="e3f51427-dcd9-4234-97f1-6681a114f4d4" class=""><details open=""><summary>Week 4</summary></details></h1><div class="indented"><p id="16eda0b8-32bb-4a82-a544-dad09271cb09" class="">this week we use the Reformer (reversible transformer) model to build a chatbot</p><p id="9cf37901-5d26-47da-8fab-e7c6266574ff" class="">we deal with tasks with long sequences and learn how to deal with them</p><p id="dc1eb234-ba57-4035-960b-1952999f6aa8" class="">
</p><p id="75f14ced-ae04-494a-8f2f-989c24b5d3db" class=""><mark class="highlight-teal"><strong>Tasks with Long Sequences and Transformer Complexity</strong></mark></p><figure id="302d2b26-8ba1-4380-b897-893797cb4b14" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2062.png"><img style="width:854px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2062.png"/></a></figure><figure id="0b3f1d01-7005-4904-b1f8-a9f7b516c0f1" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2063.png"><img style="width:2542px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2063.png"/></a></figure><p id="80571cf6-2a9c-46e9-8acd-3945504f779a" class="">When you are handling long sequences, you usually don&#x27;t need to consider all L positions. You can just focus on an area of interest instead. For example, when translating a long text from one language to another, you don&#x27;t need to consider every word at once. You can instead focus on a single word being translated, and those immediately around it, by using attention.</p><figure id="389c2dbe-d567-45dd-be7c-5085a096829d" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2064.png"><img style="width:567px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2064.png"/></a></figure><p id="c9e5bcb7-ef98-4250-85d9-46e8814e1d19" class="">activations can be re-calculated to save memory but this comes with a time tradeoff</p><p id="4284e774-580d-4461-90de-7ef186b08396" class="">
</p><p id="a8ae1876-3332-49fd-bc11-65b6bc47d359" class=""><mark class="highlight-teal">Using Locality Sensitive Hashing LSH Attention to make the dot product more efficient</mark></p><p id="0bd7a54b-31b9-42b0-8c94-79572ac5d78f" class="">LSH reduce the computational cost to find the nearest neighbors</p><figure id="d6ed7a39-0989-4f0a-8fb0-11db69bdecc8" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2065.png"><img style="width:829px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2065.png"/></a></figure><figure id="c8cd2cf7-da12-4537-9955-223e64fbaebb" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2066.png"><img style="width:843px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2066.png"/></a></figure><p id="66ca57bc-c5f5-461c-8238-403cbe0f21ac" class="">
</p><p id="31b72e22-16b8-4349-9275-ce2dd1abe589" class=""><mark class="highlight-teal">Solution to long sequences, memory using </mark><mark class="highlight-teal"><strong>Reversible Layers</strong></mark></p><p id="350089e2-9d83-4406-8e9f-54bce7fe116f" class="">in any transformer we have:<div class="indented"><p id="8ad9569a-c4c7-4f24-bf2e-7ad4d69697c7" class="">1- the input. say, 1 million tokens each is represented by a 512 length vector, 2GB</p><p id="1c50e1da-6a0a-4412-a64c-2ad88c2829cd" class="">2- the model layers with are Attention and Feed Forward, each exist at least 12 times, the input and output is also 2GB</p><p id="dcec24d6-297c-4f3a-9253-1cd9546a5aad" class="">3- intermediate quantities saved in the forward pass to use later in the backward</p></div></p><p id="8f4760d5-e3cf-46ea-a525-c60a699e1bd4" class="">memory usage grows linearly with the number of layers for different architectures</p><figure id="1c6ab914-a670-424d-a446-265e7c33ec51" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2067.png"><img style="width:2552px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2067.png"/></a></figure><p id="cbb465e7-e181-4beb-b52b-4d9b6936558a" class="">
</p><p id="14b8e060-1188-4e68-affd-566d05fb7b9c" class=""><mark class="highlight-teal"><strong>Reversible Residual Layers</strong></mark></p><figure id="bc51f225-ae6b-429f-99ff-15aabcec71d1" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2068.png"><img style="width:761px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2068.png"/></a></figure><p id="aab35fad-54a5-4684-9ee8-b6d8b78cf369" class="">instead of saving the Xs we re-compute them</p><figure id="6a8dd2fa-7c6d-49ed-ad99-74949bc9b0b3" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2069.png"><img style="width:879px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2069.png"/></a></figure><p id="f4cccf2b-43b6-48de-9c80-2a6cbee0e3ab" class="">
</p><p id="3331d89a-10a6-43bb-870e-41fd68d903b6" class=""><mark class="highlight-teal">Reformer</mark></p><p id="20eef2bc-37f4-400f-a7bd-eb7c581998b2" class="">can handle up to 1 million words, combines the techniques above</p><figure id="5c386d92-b47f-4011-ba74-86cb28f6250a" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2070.png"><img style="width:879px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2070.png"/></a></figure><p id="7ade913c-da7c-4423-a179-6ac103b3b194" class="">more hashes take more time</p><figure id="2c92cbbe-6303-4d41-ad0e-45fb60d58070" class="image"><a href="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2071.png"><img style="width:758px" src="Natural%20Language%20Processing%20with%20Attention%20Models%20%20356e5853d3fc427390282507dd2328ef/Untitled%2071.png"/></a></figure></div><hr id="94e5df21-8090-4f82-9dac-81e95a8c6a6c"/><h1 id="e13eebfc-2cd9-4720-a126-c45d629d5e50" class=""><details open=""><summary><strong>This course drew from the following resources:</strong></summary></details></h1><div class="indented"><h2 id="50798642-38ec-4062-8be0-06320961f0e1" class=""><a href="https://arxiv.org/abs/1910.10683"><strong>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</strong></a><strong> (Raffel et al, 2019)</strong></h2><h3 id="84042b79-1a9b-445f-93d4-4d4fa61859e2" class=""><strong></strong><a href="https://arxiv.org/abs/2001.04451"><strong>Reformer: The Efficient Transformer</strong></a><strong> (Kitaev et al, 2020)</strong></h3><h3 id="368e1f51-d40b-41be-aa3a-b6b2837b515f" class=""><strong></strong><a href="https://arxiv.org/abs/1706.03762"><strong>Attention Is All You Need</strong></a><strong> (Vaswani et al, 2017)</strong></h3><h3 id="34542e0f-f9cc-4f8b-a816-88bb231541c9" class=""><strong></strong><a href="https://arxiv.org/pdf/1802.05365.pdf"><strong>Deep contextualized word representations</strong></a><strong> (Peters et al, 2018)</strong></h3><h3 id="ad31e704-37d6-4ace-bf3e-a854d05db968" class=""><strong></strong><a href="http://jalammar.github.io/illustrated-transformer/"><strong>The Illustrated Transformer</strong></a><strong> (Alammar, 2018)</strong></h3><h3 id="afad3622-d900-4b33-a630-0334fcc5648e" class=""><strong></strong><a href="http://jalammar.github.io/illustrated-gpt2/"><strong>The Illustrated GPT-2 (Visualizing Transformer Language Models)</strong></a><strong> (Alammar, 2019)</strong></h3><h3 id="c53d2ca7-7496-4078-a718-f3bfbcd597f3" class=""><strong></strong><a href="https://arxiv.org/abs/1810.04805"><strong>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</strong></a><strong> (Devlin et al, 2018)</strong></h3><h3 id="6b33a86f-c747-4f8d-81e0-86e7d01a10e0" class=""><strong></strong><a href="http://jalammar.github.io/how-gpt3-works-visualizations-animations/"><strong>How GPT3 Works - Visualizations and Animations</strong></a><strong> (Alammar, 2020)</strong></h3><p id="ffe12ba0-8dd9-4807-a4d9-d70cd3a1d790" class="">
</p></div><p id="2d5f6bd3-7b8b-4d5e-9d03-d4f464427447" class="">
</p></div></article></body></html>